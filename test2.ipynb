{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunks import chunks_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=chunks_maker(path=\"output_cleaned.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chunks.chunks_maker object at 0x000001603F3FA000>\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "chunks_maker.recursive_character_splitter() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_chunks\u001b[38;5;241m=\u001b[39m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_character_splitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: chunks_maker.recursive_character_splitter() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "final_chunks=chunks.recursive_character_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.makedown_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunks=chunks.recursive_character_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import RetrievalSystem\n",
    "from app.rag import RAGPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bored\\Music\\MLMini2\\app\\retrieval.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(\n",
      "c:\\Users\\bored\\Music\\MLMini2\\textenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "retrival=RetrievalSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D91A1160F0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival.build_vector_store(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7204ff9e-3a85-449c-96b2-08bb3b65e4ee', metadata={'Header1': 'Attention Is All You Need', 'Header2': '6 Results'}, page_content='in different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head'),\n",
       " Document(id='10fa6d5f-75d2-467a-ba86-f6bea0cabf38', metadata={'Header1': 'Attention Is All You Need', 'Header2': '6 Results'}, page_content='attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to'),\n",
       " Document(id='a4eba747-90a0-40ca-aa3d-2263b286d946', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival.retrieve(\"what are evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bored\\Music\\MLMini2\\app\\rag.py:43: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  return LLMChain(\n"
     ]
    }
   ],
   "source": [
    "rag=RAGPipeline(retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bored\\Music\\MLMini2\\app\\rag.py:56: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return self.llm_chain.run(context=context, question=question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The evaluation metrics for the Transformer architecture, as mentioned in the provided context, are primarily based on the English-to-German translation development set, newstest2013. The specific metrics mentioned include:\\n\\n1. **BLEU Score**: This is a measure of the quality of the translation, with higher scores indicating better translation quality. The context mentions that attention is 0.9 BLEU worse than the best setting, indicating that the BLEU score is used to evaluate the effectiveness of different attention mechanisms.\\n\\n2. **Perplexity**: This is a measure of how well the model is able to predict the next word in a sequence, given the context of the previous words. The perplexity values mentioned are per-wordpiece, according to the byte-pair encoding used. It is noted that these perplexity values should not be compared to those from other models that use different encoding schemes.\\n\\n3. **Training Costs**: The context also mentions estimating the number of floating-point operations used to train a model, which is done by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU. This metric is used to compare the training efficiency of different model architectures.\\n\\n4. **Translation Quality**: This is a broader metric that encompasses the overall performance of the model in translating text from one language to another. The context mentions comparing the translation quality of the Transformer architecture to other model architectures from the literature.\\n\\nThese metrics are used to evaluate the performance of the Transformer architecture and its variations, as well as to compare its performance to other models. They provide a comprehensive understanding of the model's strengths and weaknesses, and help to identify areas for improvement.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.generate_answer(\"what are the evaluation metrics for transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunks import chunks_maker\n",
    "chunks=chunks_maker(path=\"output_cleaned.md\")\n",
    "chunks.makedown_splitter()\n",
    "final_chunks=chunks.recursive_character_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import RetrievalSystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002439BB6B5F0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=RetrievalSystem()\n",
    "retriever.build_vector_store(final_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='326ca4d8-fc1c-48fd-9076-b438cfecc17c', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       " Document(id='e62444f8-2746-404d-8081-32c0115cfcd2', metadata={'Header1': 'Attention Is All You Need', 'Header2': '2 Background'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       " Document(id='d19db370-71e5-40b2-827e-ca296fd5e9fa', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='Where the projections are parameter matrices Wi[Q] _∈_ R[d][model][×][d][k], Wi[K] _∈_ R[d][model][×][d][k], Wi[V] _∈_ R[d][model][×][d][v]\\nand W _[O]_ _∈_ R[hd][v][×][d][model].\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\n_dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost_\\nis similar to that of single-head attention with full dimensionality.\\n**3.2.3** **Applications of Attention in our Model**')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(\"What is attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "\n",
    "\n",
    "class DocumentRetrievalChain(Chain):\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"query\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [\"documents\"]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = inputs[\"query\"]\n",
    "        docs = retriever.retrieve(query)\n",
    "        return {\"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_docs=DocumentRetrievalChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is attention',\n",
       " 'documents': [Document(id='326ca4d8-fc1c-48fd-9076-b438cfecc17c', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       "  Document(id='e62444f8-2746-404d-8081-32c0115cfcd2', metadata={'Header1': 'Attention Is All You Need', 'Header2': '2 Background'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       "  Document(id='d19db370-71e5-40b2-827e-ca296fd5e9fa', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='Where the projections are parameter matrices Wi[Q] _∈_ R[d][model][×][d][k], Wi[K] _∈_ R[d][model][×][d][k], Wi[V] _∈_ R[d][model][×][d][v]\\nand W _[O]_ _∈_ R[hd][v][×][d][model].\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\n_dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost_\\nis similar to that of single-head attention with full dimensionality.\\n**3.2.3** **Applications of Attention in our Model**')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_docs.invoke({\"query\":\"what is attention\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    for image_path in images:\n",
    "        img = mpimg.imread(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageRetrievalChain(Chain):\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"documents\"]\n",
    "        \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [\"images\"]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        docs = inputs[\"documents\"]\n",
    "        figg = [\n",
    "            page\n",
    "            for page in docs\n",
    "            if \"Figure\" in page.page_content or \"Table\" in page.page_content\n",
    "        ]\n",
    "        docu = \"\"\n",
    "        for i in figg:\n",
    "            page_conn = i.page_content\n",
    "            docu += page_conn\n",
    "        patternn = r\"\\b(?:Figure|Table)\\s\\d+\\b\"\n",
    "        matches_ptrn = re.findall(patternn, docu)\n",
    "        bcd = set()\n",
    "        matchess = []\n",
    "        for amatch in matches_ptrn:\n",
    "            if amatch not in bcd:\n",
    "                matchess.append(amatch)\n",
    "                bcd.add(amatch)\n",
    "        image_references = list(\n",
    "            map(lambda x: \"extracted_data_fugg/\" + x + \".jpg\", matchess)\n",
    "        )\n",
    "\n",
    "        return {\"images\": image_references}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode_Images(Chain):\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "      \"\"\"Input keys.\"\"\"\n",
    "      return [\"documents\",\"images\", \"query\"]\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Output keys.\"\"\"\n",
    "        return [\"Encoded_Images\"]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        docs = inputs[\"documents\"]\n",
    "        query = inputs[\"query\"]\n",
    "        images=inputs['images']\n",
    "\n",
    "        Encoded_Images=[]\n",
    "        for path in images:\n",
    "          with open(path, \"rb\") as img_file:\n",
    "              base64_str = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "              Encoded_Images.append(base64_str)\n",
    "        return {\"Encoded_Images\": Encoded_Images}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Encodes_images\": encodes_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Encodes_images\": encodes_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMChain(Chain):\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Input keys.\"\"\"\n",
    "        return [\"documents\",\"Encoded_Images\", \"query\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Output keys.\"\"\"\n",
    "        return [\"response\"]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        docs = inputs[\"documents\"]\n",
    "        query = inputs[\"query\"]\n",
    "        Encodes_images=inputs['Encoded_Images']\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an excellent professor of AI, often compared to the greats like Feynman.\n",
    "        explain every little detail that you find in context which is relevant to the question\n",
    "        Use the following context to answer the question:\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm(messages)\n",
    "\n",
    "        return {\"response\": (response.content,Encodes_images)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_chain = DocumentRetrievalChain()\n",
    "image_retrieval_chain = ImageRetrievalChain()\n",
    "encode_images=Encode_Images()\n",
    "llm_chain = LLMChain()\n",
    "pipeline = SequentialChain(\n",
    "    chains=[retrieval_chain, image_retrieval_chain,encode_images, llm_chain],\n",
    "    input_variables=[\"query\"],\n",
    "    output_variables=[\"response\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_images(enncodes):\n",
    "  for encode in enncodes:\n",
    "    with open(\"decoded_image.jpg\", \"wb\") as file:\n",
    "      file.write(base64.b64decode(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context provided, the evaluation of the Transformer model is based on two main metrics: BLEU score and perplexity.\n",
      "\n",
      "1. BLEU score: BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine-generated translations by comparing them to human-generated references. A higher BLEU score indicates a better translation quality. In this context, it is mentioned that the attention mechanism is 0.9 BLEU worse than the best setting. This means that the translation quality of the model with the attention mechanism is not as good as the best-performing model. It is also mentioned that quality drops off with too many heads, suggesting that having a large number of attention heads may negatively impact the translation performance.\n",
      "\n",
      "2. Perplexity: Perplexity is a measurement of how well a probability model predicts a sample. In natural language processing and machine translation, perplexity is often used to evaluate language models. A lower perplexity score indicates that the model is better at predicting the sample. Here, perplexity is reported for the English-to-German translation development set, newstest2013, using a byte-pair encoding approach. The perplexity values should not be compared directly to those reported in [9] due to differences in encoding methods.\n",
      "\n",
      "In addition to these metrics, the context also provides information on the training costs of the Transformer model, estimated by multiplying the training time, the number of GPUs used, and the sustained single-precision floating-point capacity of each GPU [5]. This information is used to compare the Transformer model's performance and efficiency to other model architectures from the literature (Table 2).\n",
      "\n",
      "Lastly, the Transformer model itself is evaluated by varying its base model, as described in Section 6.2. This involves changing different components of the model, such as the number of layers, the number of heads in self-attention, and the dimensionality of the feed-forward networks, to understand the importance of each component in the overall performance of the model.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the evaluation used to evaluate transformer and explain me\"\n",
    "result = pipeline.run({\"query\": query})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "images=result[1]\n",
    "decode_images(images)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\bored\\\\Music\\\\MLMini2\\\\app'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(paths):\n",
    "    encodes_images=[]\n",
    "    for path in paths:\n",
    "\n",
    "      with open(path, \"rb\") as img_file:\n",
    "          base64_str = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "          encodes_images.append(base64_str)\n",
    "    return encodes_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds=encode_images([\"Table 3.jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_images(encodeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
