{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunks import chunks_maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=chunks_maker(path=\"output_cleaned.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chunks.chunks_maker object at 0x000001603F3FA000>\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "chunks_maker.recursive_character_splitter() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_chunks\u001b[38;5;241m=\u001b[39m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursive_character_splitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: chunks_maker.recursive_character_splitter() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "final_chunks=chunks.recursive_character_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.makedown_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunks=chunks.recursive_character_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import RetrievalSystem\n",
    "from app.rag import RAGPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bored\\Music\\MLMini2\\app\\retrieval.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(\n",
      "c:\\Users\\bored\\Music\\MLMini2\\textenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "retrival=RetrievalSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D91A1160F0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival.build_vector_store(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7204ff9e-3a85-449c-96b2-08bb3b65e4ee', metadata={'Header1': 'Attention Is All You Need', 'Header2': '6 Results'}, page_content='in different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head'),\n",
       " Document(id='10fa6d5f-75d2-467a-ba86-f6bea0cabf38', metadata={'Header1': 'Attention Is All You Need', 'Header2': '6 Results'}, page_content='attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to'),\n",
       " Document(id='a4eba747-90a0-40ca-aa3d-2263b286d946', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival.retrieve(\"what are evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bored\\Music\\MLMini2\\app\\rag.py:43: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  return LLMChain(\n"
     ]
    }
   ],
   "source": [
    "rag=RAGPipeline(retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bored\\Music\\MLMini2\\app\\rag.py:56: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return self.llm_chain.run(context=context, question=question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The evaluation metrics for the Transformer architecture, as mentioned in the provided context, are primarily based on the English-to-German translation development set, newstest2013. The specific metrics mentioned include:\\n\\n1. **BLEU Score**: This is a measure of the quality of the translation, with higher scores indicating better translation quality. The context mentions that attention is 0.9 BLEU worse than the best setting, indicating that the BLEU score is used to evaluate the effectiveness of different attention mechanisms.\\n\\n2. **Perplexity**: This is a measure of how well the model is able to predict the next word in a sequence, given the context of the previous words. The perplexity values mentioned are per-wordpiece, according to the byte-pair encoding used. It is noted that these perplexity values should not be compared to those from other models that use different encoding schemes.\\n\\n3. **Training Costs**: The context also mentions estimating the number of floating-point operations used to train a model, which is done by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU. This metric is used to compare the training efficiency of different model architectures.\\n\\n4. **Translation Quality**: This is a broader metric that encompasses the overall performance of the model in translating text from one language to another. The context mentions comparing the translation quality of the Transformer architecture to other model architectures from the literature.\\n\\nThese metrics are used to evaluate the performance of the Transformer architecture and its variations, as well as to compare its performance to other models. They provide a comprehensive understanding of the model's strengths and weaknesses, and help to identify areas for improvement.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.generate_answer(\"what are the evaluation metrics for transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunks import chunks_maker\n",
    "chunks=chunks_maker(path=\"output_cleaned.md\")\n",
    "chunks.makedown_splitter()\n",
    "final_chunks=chunks.recursive_character_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import RetrievalSystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002439BB6B5F0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=RetrievalSystem()\n",
    "retriever.build_vector_store(final_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='326ca4d8-fc1c-48fd-9076-b438cfecc17c', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       " Document(id='e62444f8-2746-404d-8081-32c0115cfcd2', metadata={'Header1': 'Attention Is All You Need', 'Header2': '2 Background'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       " Document(id='d19db370-71e5-40b2-827e-ca296fd5e9fa', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='Where the projections are parameter matrices Wi[Q] _∈_ R[d][model][×][d][k], Wi[K] _∈_ R[d][model][×][d][k], Wi[V] _∈_ R[d][model][×][d][v]\\nand W _[O]_ _∈_ R[hd][v][×][d][model].\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\n_dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost_\\nis similar to that of single-head attention with full dimensionality.\\n**3.2.3** **Applications of Attention in our Model**')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(\"What is attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval import RetrievalSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pineconeNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pinecone-6.0.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\bored\\music\\mlmini2\\textenv\\lib\\site-packages (from pinecone) (2025.1.31)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\bored\\music\\mlmini2\\textenv\\lib\\site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\bored\\music\\mlmini2\\textenv\\lib\\site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\bored\\music\\mlmini2\\textenv\\lib\\site-packages (from pinecone) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bored\\music\\mlmini2\\textenv\\lib\\site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Downloading pinecone-6.0.1-py3-none-any.whl (421 kB)\n",
      "   ---------------------------------------- 0.0/421.4 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/421.4 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/421.4 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 112.6/421.4 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 215.0/421.4 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 266.2/421.4 kB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 348.2/421.4 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  419.8/421.4 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 421.4/421.4 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone\n",
      "Successfully installed pinecone-6.0.1 pinecone-plugin-interface-0.0.7\n"
     ]
    }
   ],
   "source": [
    "%pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_72G472_U2LsZUw1eAsKbjRSb2rWzbNei1obqos2XTzGzHh33uTQHfgF3fQz1bGzEEgzwUo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#kns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=\"AIzaSyA-HZUAkWRKIhEkeTXkWixOtyUYVe5NIlE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RetrievalSystem:\n",
    "    def __init__(self,embeddings_model):\n",
    "        \"\"\"\n",
    "        Initialize the retrieval system with embedding model and search parameters.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Name of the HuggingFace embedding model\n",
    "            model_kwargs: Arguments for the embedding model\n",
    "            search_kwargs: Retrieval parameters (e.g., number of results to return)\n",
    "        \"\"\"\n",
    "\n",
    "        self.embedding_model = embeddings_model\n",
    "        self.search_kwargs = {\"k\": 3}\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "\n",
    "    def build_vector_store(self, chunks: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Create FAISS vector store from text chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of text documents to index\n",
    "        \"\"\"\n",
    "        # Convert text chunks to Document objects\n",
    "        \n",
    "        # Create FAISS vector store\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        # Create retriever\n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs=self.search_kwargs\n",
    "        )\n",
    "\n",
    "        return self.retriever\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Retrieve documents relevant to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant documents\n",
    "        \"\"\"\n",
    "        if not self.retriever:\n",
    "            raise ValueError(\"Vector store not initialized. Call build_vector_store first.\")\n",
    "            \n",
    "        return self.retriever.invoke(query)\n",
    "\n",
    "    def save_index(self, path: str) -> None:\n",
    "        \"\"\"Save FAISS index to disk\"\"\"\n",
    "        if self.vector_store:\n",
    "            self.vector_store.save_local(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_index(cls, path: str, embedding_model: Any) -> 'RetrievalSystem':\n",
    "        \"\"\"\n",
    "        Load FAISS index from disk\n",
    "        \n",
    "        Args:\n",
    "            path: Path to saved index\n",
    "            embedding_model: Embedding model instance\n",
    "            \n",
    "        Returns:\n",
    "            RetrievalSystem instance\n",
    "        \"\"\"\n",
    "        instance = cls()\n",
    "        instance.vector_store = FAISS.load_local(path, embedding_model)\n",
    "        instance.retriever = instance.vector_store.as_retriever()\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000027EAADE3140>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=RetrievalSystem(embeddings)\n",
    "retriever.build_vector_store(final_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e656d3f5-c028-4871-985c-b2837ca84069', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       " Document(id='5367c245-5cac-49a9-b05e-59832f7e94d7', metadata={'Header1': 'Attention Is All You Need', 'Header2': '2 Background'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       " Document(id='d43ca005-7744-4e3b-96a0-c9eb4d04c62d', metadata={'Header1': 'Attention Is All You Need', 'Header2': '4 Why Self-Attention'}, page_content='considerably, to O(k · n · d + n · d[2]). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(\"What is attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "class DocumentRetrievalChain(Chain):\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"query\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [\"documents\"]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        query = inputs[\"query\"]\n",
    "        docs = retriever.retrieve(query)\n",
    "        return {\"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_docs=DocumentRetrievalChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is attention',\n",
       " 'documents': [Document(id='e656d3f5-c028-4871-985c-b2837ca84069', metadata={'Header1': 'Attention Is All You Need', 'Header2': '3 Model Architecture'}, page_content='**3.2** **Attention**\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n**3.2.1** **Scaled Dot-Product Attention**\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       "  Document(id='5367c245-5cac-49a9-b05e-59832f7e94d7', metadata={'Header1': 'Attention Is All You Need', 'Header2': '2 Background'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].'),\n",
       "  Document(id='d43ca005-7744-4e3b-96a0-c9eb4d04c62d', metadata={'Header1': 'Attention Is All You Need', 'Header2': '4 Why Self-Attention'}, page_content='considerably, to O(k · n · d + n · d[2]). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_docs.invoke({\"query\":\"what is attention\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    for image_path in images:\n",
    "        img = mpimg.imread(image_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageRetrievalChain(Chain):\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"documents\"]\n",
    "        \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [\"images\"]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        docs = inputs[\"documents\"]\n",
    "        figg = [\n",
    "            page\n",
    "            for page in docs\n",
    "            if \"Figure\" in page.page_content or \"Table\" in page.page_content\n",
    "        ]\n",
    "        docu = \"\"\n",
    "        for i in figg:\n",
    "            page_conn = i.page_content\n",
    "            docu += page_conn\n",
    "        patternn = r\"\\b(?:Figure|Table)\\s\\d+\\b\"\n",
    "        matches_ptrn = re.findall(patternn, docu)\n",
    "        bcd = set()\n",
    "        matchess = []\n",
    "        for amatch in matches_ptrn:\n",
    "            if amatch not in bcd:\n",
    "                matchess.append(amatch)\n",
    "                bcd.add(amatch)\n",
    "        image_references = list(\n",
    "            map(lambda x: \"extracted_data_fugg/\" + x + \".jpg\", matchess)\n",
    "        )\n",
    "\n",
    "        return {\"images\": image_references}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode_Images(Chain):\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "      \"\"\"Input keys.\"\"\"\n",
    "      return [\"documents\",\"images\", \"query\"]\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Output keys.\"\"\"\n",
    "        return [\"Encoded_Images\"]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        docs = inputs[\"documents\"]\n",
    "        query = inputs[\"query\"]\n",
    "        images=inputs['images']\n",
    "\n",
    "        Encoded_Images=[]\n",
    "        for path in images:\n",
    "          with open(path, \"rb\") as img_file:\n",
    "              base64_str = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "              Encoded_Images.append(base64_str)\n",
    "        return {\"Encoded_Images\": Encoded_Images}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (1565838695.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"Encodes_images\": encodes_images\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "\"Encodes_images\": encodes_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (1565838695.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"Encodes_images\": encodes_images\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "\"Encodes_images\": encodes_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMChain(Chain):\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Input keys.\"\"\"\n",
    "        return [\"documents\",\"Encoded_Images\", \"query\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Output keys.\"\"\"\n",
    "        return [\"response\"]\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        docs = inputs[\"documents\"]\n",
    "        query = inputs[\"query\"]\n",
    "        Encodes_images=inputs['Encoded_Images']\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an excellent professor of AI, often compared to the greats like Feynman.\n",
    "        explain every little detail that you find in context which is relevant to the question\n",
    "        Use the following context to answer the question:\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        messages = [HumanMessage(content=prompt)]\n",
    "        response = llm(messages)\n",
    "\n",
    "        return {\"response\": (response.content,Encodes_images)}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_chain = DocumentRetrievalChain()\n",
    "image_retrieval_chain = ImageRetrievalChain()\n",
    "encode_images=Encode_Images()\n",
    "llm_chain = LLMChain()\n",
    "pipeline = SequentialChain(\n",
    "    chains=[retrieval_chain, image_retrieval_chain,encode_images, llm_chain],\n",
    "    input_variables=[\"query\"],\n",
    "    output_variables=[\"response\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_images(enncodes):\n",
    "  for encode in enncodes:\n",
    "    with open(\"decoded_image.jpg\", \"wb\") as file:\n",
    "      file.write(base64.b64decode(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bored\\AppData\\Local\\Temp\\ipykernel_22112\\3586001984.py:31: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation used to evaluate the Transformer model, as mentioned in the context, involves varying the base model to assess the importance of different components. This is described in section 6.2 of the text. The Transformer model is compared to other model architectures from the literature in terms of translation quality and training costs. The number of floating point operations used to train a model is estimated by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.\n",
      "\n",
      "In addition to this, the Transformer model is evaluated based on its architecture, which is a transduction model that relies on self-attention mechanisms rather than recurrent neural networks. The Transformer follows an overall architecture of using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. The encoder is composed of a stack of identical layers, each having two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder also follows a similar architecture but with an additional sub-layer that performs multi-head attention over the output of the encoder.\n",
      "\n",
      "The Transformer model's performance is also evaluated on simple-language question answering and language modeling tasks, where it has been shown to perform well, outperforming other models like end-to-end memory networks that are based on a recurrent attention mechanism.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the evaluation used to evaluate transformer and explain me\"\n",
    "result = pipeline.run({\"query\": query})\n",
    "\n",
    "images=result[1]\n",
    "decode_images(images)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\bored\\\\Music\\\\MLMini2\\\\app'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(paths):\n",
    "    encodes_images=[]\n",
    "    for path in paths:\n",
    "\n",
    "      with open(path, \"rb\") as img_file:\n",
    "          base64_str = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "          encodes_images.append(base64_str)\n",
    "    return encodes_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeds=encode_images([\"Table 3.jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_images(encodeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ImageRetrievalChain' from partially initialized module 'rag2' (most likely due to a circular import) (c:\\Users\\bored\\Music\\MLMini2\\app\\rag2.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrag2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\bored\\Music\\MLMini2\\app\\rag2.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchunks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chunks_maker\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mretrieval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalSystem\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RAGPipeline\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Any\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chain\n",
      "File \u001b[1;32mc:\\Users\\bored\\Music\\MLMini2\\app\\app.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrag2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageRetrievalChain,DocumentRetrievalChain,Encode_Images,LLMChain\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuestionRequest,AnswerResponse\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastAPI, HTTPException\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ImageRetrievalChain' from partially initialized module 'rag2' (most likely due to a circular import) (c:\\Users\\bored\\Music\\MLMini2\\app\\rag2.py)"
     ]
    }
   ],
   "source": [
    "from rag2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
